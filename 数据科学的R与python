##概述
和那些数据科学比赛不同，在真实的数据科学中，我们可能更多的时间不是在做算法的开发，而是对需求的定义和数据的治理。
所以，如何更好的结合现实业务，让数据真正产生价值成了一个更有意义的话题。

数据科学项目的完整流程通常是这样的五步骤：
需求定义=》数据获取=》数据治理=》数据分析=》数据可视化

##1.需求定义
需求定义是数据科学项目和数据科学比赛的最大不同之处，在真实情景下，我们往往对目标函数、自变量、约束条件都并不清晰。
需要通过访谈、论文、文档等等形式对问题进行系统地分析，将实际问题量化为可以解决的抽象问题，确定自变量、约束条件以及目标函数。
在真实情景下，需求往往是多变化的，临时的，如何把握好需求成为了整个项目后续推进的关键要素。

##2.数据的获取
数据获取的形式主要包括：

现有数据库的调用
现有API的调用
自行设计的爬虫
在数据获取中，比较重头的一项就是爬虫的开发，这里 R 虽然开发了 rvest 包，
不过比起 django-scrapy 这样完整的爬虫调度系统不禁黯然失色，所以在第一步，我建议使用Python做爬虫的开发。

##3.数据的治理
数据治理第一步就是数据的定义，而数据的定义通过 Python的各种ORM框架和admin系统，可以非常出色地完成数据仓库的定义和管理。
通过 airflow 我们又可以很好的对ETL过程做全流程的监控。
所以，在第二步，我依然建议使用Python作为数据治理的工具。

##数据分析
数据分析首先涉及的就是探索式分析，这一点正是R语言的强项，适宜于各种强大的数据可视化，我们可以利用R快速了解数据的整体特性，
通过 data.table 和 Rcpp 我们也可以快速提升 R 的单机性能，省去了Cython写wrapper的尴尬.
而Python 由于需要更多约束的分析操作，在探索式分析中相比 R 少了几分灵活性。
至少是矩阵乘法我更愿意接受直观的 %*%, 而不是np.dot()。所以，第三步，我建议使用 R 完成数据的分析工作。

##数据的可视化
数据可视化本来是JS的天下，但是感谢 R语言生态中热衷于给JS做封装的开发者们，
现在市面上绝大部分在BI领域会涉及到的JS库都已经被 R 语言封装好了，比如 echarts、highcharts、rcharts、d3等等。另一方面，
通过 shiny， 我们快速极大地简化了BI构建的流程，跳过底层jquery、boostrap、websocket等等细节，直接针对业务场景构建BI系统，
帮助我们在快速构建BI原型上扫清障碍，而不是在Tornado里面辛苦地改template。显然，使用 R 做数据可视化可以大大减少我们的开发时间。
所以，第四部，我也建议使用 R 完成数据可视化的工作

##总结
这样正常数据科学项目做下来，
我们需要交付一个爬虫管理系统（django-scrapy）、一个数据仓库管理系统(django)、一个流程监控系统(airflow)、一个BI分析系统(shiny)，
真正完成对整个数据科学项目的可监控、可维护，然后在这个过程中我们不断去迭代我们的数据产品，优化流程，完善模型，最终实现反哺业务。

总结起来，将Python作为数据科学的基础，而R作为上层建筑是一个不错的解决方案，当然这一切都建立在数据开发人员具有过硬的开发技能，
否则Python和R的随意性会酿成巨大的惨案
